[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classification trees in CODAP using Arbor",
    "section": "",
    "text": "Arbor is a CODAP plugin that helps you make classification trees. If you are in a workshop or a class, your teacher will probably tell you a lot of what’s here. You can use this site as a reference, to remember what you’ve forgotten, or as a stand-alone self-paced course, in which case you can just work through the headings you see at left.\n\nOverview\n\nAnatomy of a tree\n\nLearn how to read a tree, using data from the Titanic disaster.\n\nFirst driving lesson\n\nThe basics of how to make a tree using Arbor.\n\nTrees and graphs\n\nTrees and graphs are both ways of looking at the data. And they are intimately connected. Learn how to use graphs to help you make th ebest trees.\n\nFbola example\n\nAnother example dataset, this time in a medical context. Use the tree to decide who probably has the virus.\n\nTree quality\n\nWhat makes a tree a good tree? Arbor can help you calculate tree quality.\n\nBreast cancer example\n\nWith a numerical predictor, we create our own measure of tree quality, and use it with graphs to make the best tree.\n\nMachine learning connection\n\nClassification is an important topic in machine learning. This explains the connection and how work with Arbor might fit in.\n\nThe configuration box\n\nWhen you “configure” a node in a tree, there is a dialog box. This explains all of its controls.\n\n\nTo learn why you would want to use this plugin, see xxx."
  },
  {
    "objectID": "configuration.html",
    "href": "configuration.html",
    "title": "Configuring a node: the details",
    "section": "",
    "text": "When you press the “gear” that appears when you hover over a node, the configuration box appears. That’s where you can set the cutpoint for a numeric attribute.\n\n\n\n\n\nThe gear appears when you hover over a node.\n\n\nYou can do other useful things as well.\nYou will be able to figure this box out pretty well on your own, but just in case, this page explains all the gory details.\n\nLeft and right\nThe double-headed arrow diamond near the lower right is a button that reverses the node. With that control, you can (for example) set up every branching so that the more “positive” result flows to the left. A tree is less confusing if the results are less mixed up.\n\n\nCutpoints\n\n\n\nSample configuration box for age\n\n\nIf the attribute is numeric, you have to decide what value separates the positive from the negative values.\nEnter the value you want for the cutpoint and use the menu to choose the operator that governs which value(s) go on the left.\nNotice that you can use equality. This is useful when you want to isolate a single value for some reason.\n\n\nMore than two categorical values\nSuppose you have four values in the columns, such as Freshman, Sophomore, Junior, and Senior. By default, Arbor picks one value for the left side, and puts the rest on the right:\n\n\n\n“Before.” Frosh are alone on the left.\n\n\nNow suppose you want to split a node by whether the cases are upper or lower class.\nJust click on a value to move it to the other side. For example, clicking on the Soph button will move it to the left side. It will look like this, although you have to edit the labels as we did:\n\n\n\n“After.” Frosh and sophomores are together on the left."
  },
  {
    "objectID": "with-graphs.html",
    "href": "with-graphs.html",
    "title": "Trees and graphs",
    "section": "",
    "text": "How should you decide how to build a tree? What attributes should you drag in? How do you pick a cut point?\nYou could just try things, but it often helps to make graphs: graphs that show the relationship between the attribute you’re wondering about and your target variable.\nFor example, here are two graphs of the same data showing the height of 28 professional athletes and what their sport is. In this case, all of these athletes play either rugby or basketball. That attribute, sport, is our target variable. That is, we are trying to use height to predict what sport they play.\n\n\n\n\n\n\nusing the vertical axis\n\n\n\n\n\n\n\nusing a legend\n\n\n\n\n\n\n\nTwo ways to show sport and height. As we might expect, basketball players are generally taller.\nWhich display is best? You will have to decide what works for you. Be sure you remember that you have a choice!\nWe could use either graph to decide on a cut point. It looks as if it’s something like 175 cm.\nIn the live example make a tree…\n\n…that’s predicting sport\n…that branches on height\n…at a number near 175 cm\n…and that predicts that the taller athletes play basketball, and the rest rugby."
  },
  {
    "objectID": "begin.html",
    "href": "begin.html",
    "title": "Driving Arbor: the very basics",
    "section": "",
    "text": "Let’s learn to drive! We will begin by exploring a dataset about the passengers aboard the Titanic. This is the goal:\n\n\n\nYou will make this tree showing data about passengers on the Titanic. Notice the huge gender difference in survival rates.\n\n\n\nThe target attribute\nTo make a tree, you first have to identify your target attribute. The target attribute might also be called\n\nthe dependent variable\nthe outcome variable\nthe effect\n\nIt’s the thing you are trying to predict. In the case of the Titanic data, it’s fate: we want to know what makes it more likely to have survived the disaster.\nIn the live example below, start your tree by dragging fate from the table at the right and dropping it into the middle of the blank, gray tree panel. You should see that 500 of the 1309 people survived.\n\n\n\n\n\nMaking a branch: drag and drop\nNext, drop sex onto the white box with “500 of 1309” in it.\nThe tree will branch. You can see how the survival rate was different for males and females.\nNotice that the tree is composed of boxes (called nodes) and lines (links).\n\n\n\n\n\n\nLost in the Node?\n\n\n\nIt’s easy to lose track of what’s going on in a node. When that happens, just point at the node. Don’t click, just hover for a moment, and text will pop up describing that node in more detail.\n\n\n\n\n\n\n\nThe pop-up information for the “male” node\n\n\n\n\nAdding a numeric attribute\nNow drop age onto the “male” node. It will split, but probably using age 30 as a cutpoint. Click the gear on the age stripe and change that 30 to 15.\n\n\n\n\n\nConfiguring age: setting the cutpoint to 15.\n\n\nWhen you make a tree with Arbor, every node has a maximum of two branches. An attribute like age has so many values, you generally have to tell Arbor where that cutpoint is using that configuration box.\nYou can configure any attribute, but it’s more common for numerical ones.\n\n\nAssigning results to terminal nodes\n\n\n\n\n\nYour tree should look like this after you have assigned its leaf nodes to be survived or died.\n\n\nNow your tree should look like the one up at the top of the page… except that you still have to assign the terminal nodes — the leaves of the tree — to an outcome. Do that by clicking on the leaf nodes repeatedly until you see what you want."
  },
  {
    "objectID": "anatomy.html",
    "href": "anatomy.html",
    "title": "Anatomy of a tree",
    "section": "",
    "text": "What does an Arbor tree look like? How do you read it? Let’s look at an example. This tree shows data from a famous dataset about the 1309 passengers aboard the Titanic. We will explore what seems to affect whether the passengers survived.\n\n\n\nLater, you will make this tree!\n\n\nFirst of all, this tree—like most trees in data analysis—is upside-down. The root and trunk are at the top, and the leaves are at the bottom.\n\nThe root node\nSo: way up at the top, the “root” block, the root node, looks like this:\n\n\n\n\n\nThe first horizontal “stripe” tells us what attribute we are trying to predict. In this dataset, it’s called fate. (The value of fate is either survived or died.)\nThe second stripe tells us that we will consider survived to be the “positive” result. That seems obvious, but in many medical contexts, a “positive” test means that you have the disease.\n\n\nThe trunk node\nJust below the root is the trunk. That block, that node, also has two stripes.\nThe top one tells you that 500 of the 1309 people—38.2% of them—survived. You know it’s survived because of what it says in the root. But if you forget, or get confused, you can always hover over that node to see details:\n\n\n\n\n\nThe bottom stripe sets up a branching. In this case, we ask about gender, and branch one way if the person is female and the other way if they are male.\n\n\nOther nodes\nEvery other node is structured more or less the same as the trunk node: the first stripe shows how many people are positive, that is, how many survived, and the second shows the branching if there is one.\nThe key thing about the number stripe is that it only considers the people who get to that point in the tree. So although 38% of all people survived, the tree shows us that there is a big gender difference: 73% of the females survived, but only 19% of the males.\nAnd then, among the males, the young males—under 15 years old—had a 50% survival rate compared to about 18% for males 15 or older.\n\n\n\n\n\n\n\nTerminal nodes: the leaves\nIf a node has no branches coming out of it, it is a terminal node. Every terminal node has a “leaf” below it that is rounded instead of rectangular. The leaf shows a prediction for the fate of the people in that category.\nSo for the females, our best guess is that they will survive. For older males, our best guess is that they will die.\nFor the young males, under 15, with a 50% survival rate, we’re predicting survival, although you can make a case that at 50% we shouldn’t predict. The other side of that argument is that they have a better chance than people in general (38%).\n\n\nThe links\nSlanted white stripes, called links, connect nodes to their “children.” Every link has a label such as female or >= 15 so you can tell who that link applies to.\nSo: that’s how you read a tree! To learn how to make a tree, see the page about learning to drive."
  },
  {
    "objectID": "measures.html",
    "href": "measures.html",
    "title": "How good is your tree?",
    "section": "",
    "text": "Now we come to the issue of the quality of a tree. If you’ve made a tree, how good is it? And more deeply, if you have two versions of a tree, which is better?\nOf course, if a tree is perfect—it gives a correct diagnosis every time—there is no problem deciding if it’s good enough. It is. But if you’ve stayed with us this far, you have seen that the prediction a tree gives is not always correct.\nSo: to compare two trees, we need a measure of tree quality. You will pick the tree with the best value for that measure.\nThe problem is, what measure should you pick? With Arbor, you can construct various measures and then see how they evaluate the trees. You can ask yourself whether the measure you choose (or design) faithfully represents what you think is important in your context.\n\nConstructing a measure\nLet’s look at one common measure, called the misclassification rate, which I will abbreviate MCR. That’s equal to\n\\[\\rm{MCR} = \\frac{\\textrm{number of wrong diagnoses}}{\\textrm{number of diagnoses}}\\]\nArbor can supply you with various numbers you can combine to make your measure. These include the number of true positives (\\(\\rm{TP}\\)), false negatives (\\(\\rm{FN}\\)), and so forth. With those numbers, you could calculate the MCR like this:\n\\[\\rm{MCR} = \\frac{FP+FN}{TP+TN+FP+FN}\\]\nThe plan, then, would be to make a formula for MCR—or whatever measure you want—in CODAP. But where would you make that formula? And how would you get the attributes like TP to put in it?\n\n\nGetting FP and TN and all that\nAs you have seen, these numbers appear at the bottom of your tree. You could copy them, but you don’t have to.\nLet’s revisit the Fbola example. The live illustration below shows a tree that uses only the rash attribute as a predictor. In the context of the example, that means we send everybody home who has a rash, and do not take their temperature.\nAs you can see from the values below the tree, among the 100 people, we have 7 false positives (FP=7) and 12 false negatives. That means that the misclassification rate, MCR, is (7 + 12)/100, or 0.19.\n\n\n\nWe don’t want to do that calculation by hand every time, so do this:\n\nClick the disclosure triangle just left of in order to export. Some controls appear. Ignore most of them!\nPress the emit data button.\n\nAha! A new table appears called Classification Tree Records. You can see that it has already calculated MCR and (scroll right…) reports values for TP, FN, and the rest as well as N (the total number) and potentially useful quantities such as the number of nodes altogether and the depth of the tree.\nLet’s change the tree so we can compare!\n\nDrag fever in and drop it on the left-hand node.\nGive the two “vacant” leaves appropriate values.\nClick emit data again.\n\nYour new table should now look like this:\n\nAccording to the MCRs—where we want a small value—the new tree is better. The sensitivity1 (sens) in the table is also better (we want it to be large).1 We will discuss sensitivity in a bit\nNow. Arbor comes with MCR and sens pre-defined. But they are only CODAP columns with formulas. This means that you can make new columns inth is table and define any possible measure of quality for your trees.\n\n\nWhich tree made this?\nThe best for last: As you might imagine, after you’ve made a few trees and emitted the data, you might not reemmber exactly what tree created which line in the table.\nDon’t worry: Arbor has your back.\nSimply click on the row you want to know about in the Records table, and Arbor will restore that tree.\nWe will continue with this topic in a bit more depth in the next example, using breast-cancer data."
  },
  {
    "objectID": "ml-connection.html",
    "href": "ml-connection.html",
    "title": "Trees and machine learning",
    "section": "",
    "text": "One reason to learn about trees is that they are a topic in Machine Learning (ML), and that topic is a hot one right now. So thinking deeply about trees helps you understand what ML is all about.\nThis is not the place for a treatise on machine learning. But I will natter on for a bit describing some of the connections I see. If ML is new to you, it should give you an idea of at least one part of ML and where this fits.\nSo: what’s the connection between classification trees and machine learning?\nIf you have worked through all of the pages before this one, you’ve seen that, although you can make pretty good trees just using your intuition, there are ways to assign numbers to the quality of the trees you make, and informally optimize a tree. That is, you can make a plausible tree, and then alter it—add another branch, change a break point—and gradually find the best one.\nIf you do that enough, it probably will occur to you that this is tedious, and wonder if we could have a computer do it.\nThis business of looking at a set of trees, then finding the best one, and maybe then using that to make another, generally better set of trees, and repeating that process, well, that’s an example of machine learning. It’s creating an algorithm to find the best—or at least a pretty good—version of something.\n\nImagining automating tree-building\nIt’s one thing to say, we’ll have the computer do the boring part. It’s another to specify exactly what to have the computer do. We will not do that here completely, but we will talk about what’s required for that task.\n\nThe branch-everything strategy\nFirst, let’s imagine a brute-force procedure: make the biggest tree possible. To do that, take the first “predictor” attribute that you see, and branch the tree using that attribute. Now you have two terminal nodes. Then take the next attribute, and branch both terminal nodes; now you have four. Continue until you’re out of attributes. (If you start with \\(n\\) attributes, you will wind up with \\(2^n\\) terminal nodes…which can be a lot!)\nTo do that, you still need three things:\n\nAt the beginning, you need a target attribute and you need to know what value(s) of that attribute are “positive.”\nYou need to know, for each attribute, if it has more than two values, how to split it: You need a splitting rule—perhaps a cut point.\nAt the end, you need to know, for each terminal node, what diagnosis you will assign.\n\nYour algorithm will have to address those issues. Then, if you’re about to make the last branch, you will notice situations where it makes no sense to branch the tree, for example:\n\nwhen the split makes almost no difference in the percentage of cases that are positive.\nwhen the number of cases in a branch is small\n\nSo to make a “trimmer” tree, your algorithm will need to specify what “almost no difference” or “small” mean.\nYou might also look at your completed tree and decide to “prune” it—to eliminate additional branches that yield little difference or have too few cases.\n\n\nA more prudent and frugal strategy\nAn alternative to branching everything is to be more careful about what branches you make. Imagine these steps:\n\nTake all the attributes and make a tree for each one, with just that first split.\nFor each tree, calculate how good the tree is using some measure such as the MCR (misclassification rate).\nTake the best tree as measured by that MCR, returning to step 1 using only the remaining attributes.\nIf you ever get a split that’s too little difference or too small, stop branching that part of the tree.\n\nYou still need to define many things, but this will generally yield smaller, more wieldy trees.\n\n\n\nLinear regression metaphor\nLet’s assume we all understand about least-squares linear regression, a process by which, given a set of data points, we find a line that minimizes the sum of the squares of the residuals. We compute the parameters of that line—the slope and intercept, \\(m\\) and \\(b\\)—using some formulas. And those formulas get derived using calculus.11 This is not surprising, because this is an optimization problem with quadratic functions (sum of squares) so it involves setting a derivative equal to zero…and so forth.\nNow let’s imagine that we don’t have calculus.\nWe can still solve the problem using an iterative process. Maybe we begin using the function \\(m=0\\) and \\(b=0\\), that is, \\(y=0x+0\\). We can compute the sum of squares of the residuals. Then, at every step, we look at new values of \\(m\\) and \\(b\\), offset a little from their current values, and calculate the sum of squares from new lines defined by the new values. We pick the line with the lowest sum of squares and then do it all over again. Gradually, when we get close, we can reduce the offsets, and thereby find an optimum line to as great a precision as you like.\nYou can even imagine this as walking on a surface. The \\(m\\) and \\(b\\) axes define a horizontal plane, and the height (\\(z\\)) of the surface is the sum of squares for each location \\((m, b)\\). Our task is to find the lowest point on the surface.\nCalculus does it immediately, but we have the computing power to do the “walk” very efficiently and quickly. And of course, this procedure—which is called gradient descent in ML-speak—works even when calculus does not.\ncost: entropy, gini instead of SSR\ntraining and test sets\noverfitting"
  },
  {
    "objectID": "fbola.html",
    "href": "fbola.html",
    "title": "Fbola example",
    "section": "",
    "text": "A medical context can be good for learning about trees because (a) this kind of thinking is actually used in medical contexts; and (b) the thinking and terminology of trees make sense here.\nOur goal is going to be to make a diagnosis. Traditionally, “positive” means that you have the disease, and “negative” means that you do not.11 Yes, this is backwards from what makes sense logically. But it’s a tradition!\nThe tree represents a procedure for making a diagnosis. Every node represents a question or a test, and every branch an answer. When you get to the end of the tree, when you arrive at a terminal node, that node must have a diagnosis assigned to it, either positive or negative.\nClearly it’s time for an example.\nSuppose we’re at a school and there is an epidemic of the Fbola2 virus. The sympoms include a face rash and a fever. We want to evaluate students as they arrive and send them home if they are sick. Here is a tree we might use:2 Like Ebola, but not as serious.\n\n\n\nA tree we might use to diagnose students coming to school.\n\n\nThe tree makes what you’re supposed to do totally clear.\n\nThe same tree, with data\nThat tree was made from intuition. We can also make it with data, using Arbor. Suppose that we take 100 students and give them an expensive, time-consuming test for Fbola; those test results appear in the Fbola column. We also record whether they have a rash or a fever.\nUse the live example below to make the tree. Don’t hesitate to make graphs to see relationships between the attributes.\nYou can even consider making a different tree to accomplish the same task—by asking about the fever first.\nDon’t forget to include diagnoses at the ends of all your branches!\n\n\n\n\n\nTree as model\nNotice this very very important fact: our diagnosis might be wrong. A positive diagnosis using the tree might be a true positive (TP), that is, it’s correct and you have the disease; or it might be a false positive (FP), that is, we send you home even though you are well.\nSimilarly, you can get false negatives (FN) and true negatives (TN).\nYou can see all four possibilities by looking at the table tab in Arbor. (Do that!) A typical tree, with its corresponding table, look like this:\n\n\n\n\n\n\n\n\n\nThe 14 false positives combine seven from the leftmost leaf and seven from the middle one.\n\n\n\n\n\nThis leads us to an issue we have to emphasize: how do we know whether our diagnosis is right or wrong? The answer to that is, sometimes we don’t. In this case, though, we assume that the time-consuming and expensive test is perfectly accurate. In our practical situation at school, however, we hope our rash-and-fever tree does a good enough job.\nIn general, when we have a classification problem like this, there is some underlying Truth that we cannot see. We can only see the shadow3 of this Truth, in the form of data. We see the symptoms, not the actual disease. We use the data to make our best guess about the Truth.3 So Platonic!\nAnd in fact, look at at the tree in Arbor. We are trying to predict Fbola—the results of the expensive test—using the data about rash and fever.\nTaken together, this all means that a tree is a model. It’s an approximation of the truth that we will make as useful as possible. But it’s not the Truth; it’s a human construct.\nAlso, math nerds, notice that in this model, the tree’s procedure is a function. Its inputs are the data (fever and rash) and the inevitable output is either positive or negative. Notice how this is parallel to the situation when you use a line as an approximation to data in a scatter plot. The line is a function, and it’s not completely correct even though it can be useful.\nIn that situation, we can even try to find the best line using a criterion such as least squares. And that’s whats coming next with our lessons on trees."
  },
  {
    "objectID": "breast-cancer.html",
    "href": "breast-cancer.html",
    "title": "More about tree quality with breast cancer data",
    "section": "",
    "text": "In this section, we will further our understanding of the quality of trees, remind ourselves about what happens with numerical data, and learn a brutal truth about false negatives and false positives.\nLet us begin by saying that the two sways you can be wrong—FN and FP—are usually not equally bad. The misclassification rate (MCR) makes no distinction: for the MCR, wrong is wrong.\nBut let us look at some breast cancer data.\nWe have a number of numerical measurements of tumors that were calculated by image processing software, using digital images. We will focus here on radius, which is in millimeters. There are other measurements too such perimeter, which is obvious, and concavity, which would demand some clear explanation. And as before, we have a version of the Truth: the result from a biopsy, which in this case is either benign or malignant. The goal is to predict malignancy without the expense and wait-time of a biopsy: just take an image and let the computer figure it out.\nNow, which would you rather have as your mistake—a false negative or a false positive?\nMost of us would say that a false negative is worse. (Never mind for a moment that you can make the opposite case.)\n\nLooking at the data\nThe live illustration below is already set to predict biopsy. Notice:\n\nLook at the tree: among these 569 tumors, 37.3% were malignant.\nLook at the graph: apparently (and unsurprisingly) benign tumors are generally smaller.\nAlso in the graph: we have a “movable value” currently set to 16.6 millimeters. Almost all of the tumors bigger than 16.5 mm are malignant.\n\nDo the following:\n\nDrop radius into the trunk of the tree to make a branch.1\n\n1 The table—collapsed into “case card view”—may be hiding behind the tree. Make the tree narrower.Arbor will pick a cut point that determines which values of radius go to the larger branch and which go to the smaller. When I did this, that cut point was 26 mm. You need to change that to be 16.5.\n\nHover over the radius node and click the gear that appears. The configuration box appears.\n\n\n\n\n\n\nLearn more about the configuration box.\n\n\n\nChange the value in the box from 26 to 16.5 and press Done.\nSpecify diagnoses for your leaves: make the “large” tumors Malignant.\n\n\n\n\nIf your setup is like mine, that will give you 125 amlignant diagnoses, of which 122 were correct. So that leaves three false positives. On the otehr hand, there are 90 false negatives. Thats’ a lot!\nLet’s alter the graph to see this more clearly.\n\nDrop biopsy onto the vertical axis of the graph.\n\nThe graph splits to make two parallel dot plots, each with its own movable value. There are now four counts (and percentages); one for TP, one for FP, etc. See if you can clearly identify which is which, and why. Your graph should look like this:\n\n\n\nIf everything is working correctly, your graph should look like this.\n\n\nWe want to figure out what a good cut point would be. You can explore changing the movable values from 16.5 (you have to change both of them) and see how that affects the numbers of FP (upper right) and FN (lower left). And you should notice two sad truths:\n\nBecause the two distributions overlap, there is no way to get both FP = 0 and FN = 0.\nFalse negatives are the worst. We want to lower that number. But if you lower the cutpoint value to decrease FN, FP has to increase.\n\nSuppose we wanted to eliminate FN’s altogether. We would set the cutpoint to some small number (like zero)…but then all tumors would be diagnosed as malignant. No false negatives, because there would be no negatives at all!\nThis means that every cutpoint is a balancing act. We can use a measure of goodness of a tree to help us achieve that balance.\nWe could make a measure of our tree’s effectiveness just like we did in the previous section. Suppose we decide that a false negative is five times as bad as a false positive. Then our formula might be\nFN * 5 + FP\nand we would want to minimize that value.\nRemember that to get Arbor to calculate the values,\n\nOpen up the in order to export section below the tree.\nPress emit data.\n\nThat will give you a new table with values for FP etc. Then,\n\nMake a new attribute and give it that formula, FN * 5 + FP. The value you’ve calculated appears in the table.\nMake another new attribute and enter the value for the cut point (otherwise you might forget).\n\n\n\nSome tasks\nHere are some tasks you might do to extend your understanding.\n\nFind the minimum\nTry various cutpoints and see which one gives the lowest value for the measure.\n\nChange the value of the cutpoint using the configuration box\nRepeat the instructions above: emit data and enter the value of the cutpoint. The FN * 5 + FP will be calculated automatically.\nMake a graph of that value as a function of the cutpoint. Is there a minimum? (Yes!)\n\nNote: the movable values in the graph are not connected to the cut point!\n\n\nTry other attributes\nSee how you can do putting other attributes besides radius in the tree.\nDon’t forget to help yourself out by making graphs!"
  }
]