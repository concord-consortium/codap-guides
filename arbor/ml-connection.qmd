---
title: "Trees and  machine learning"
reference-location: margin
citation-location: margin
---

```{=html}
    <script src="src/awash-functions.js"></script>
```

One reason to learn about trees is that they are a topic in Machine Learning (ML),
and that topic is a hot one right now. 
So thinking deeply about trees helps you understand what ML is all about.

This is not the place for a treatise on machine learning. 
But I will natter on for a bit describing some of the connections I see. 
If ML is new to you, it should give you an idea of at least one part of ML and where this fits. 

So: what's the connection between classification trees and machine learning? 

If you have worked through all of the pages before this one,
you've seen that, although you can make pretty good trees just using your intuition,
there are ways to assign numbers to the quality of the trees you make,
and informally optimize a tree. 
That is, you can make a plausible tree, and then alter it---add
another branch, change a break point---and gradually find the best one.

If you do that enough, it probably will occur to you that this is tedious,
and wonder if we could have a computer do it.

This business of looking at a set of trees,
then finding the best one, 
and maybe then using that to make another, generally better set of trees,
and repeating that process,
well, that's an example of machine learning. 
It's creating an *algorithm* to find the best---or at least a pretty good---version of something. 

### Linear regression metaphor

Let's assume we all understand about least-squares linear regression,
a process by which, given a set of data points, we find a line that minimizes 
the sum of the squares of the residuals.
We compute the parameters of that line---the slope and intercept, $m$ and $b$---using some formulas.
And those formulas get derived using calculus.^[This is not surprising,
because this is an optimization problem with quadratic functions (sum of *squares*) so it
involves setting a derivative equal to zero...and so forth.]

Now let's imagine that we don't have calculus. 

We can still solve the problem using an iterative process. 
Maybe we begin using the function $m=0$ and $b=0$, that is, $y=0x+0$.
We can compute the sum of squares of the residuals. 
Then, at every step, we look at new values of $m$ and $b$, offset a little from their current values,
and calculate the sum of squares from new lines defined by the new values. 
We pick the line with the lowest sum of squares and then do it all over again.
Gradually, when we get close, we can reduce the offsets, and thereby find
an optimum line to as great a precision as you like. 

You can even imagine this as walking on a *surface*.
The $m$ and $b$ axes define a horizontal plane,
and the height ($z$) of the surface is the sum of squares for each location $(m, b)$.
Our task is to find the lowest point on the surface.

Calculus does it immediately,
but we have the computing power to do the "walk" very efficiently and quickly.
And of course, this procedure---which is called *gradient descent* in ML-speak---works even when calculus does not. 



cost: entropy, gini instead of SSR

training and test sets

overfitting
