[
  {
    "objectID": "00-overview.html",
    "href": "00-overview.html",
    "title": "2  Overview",
    "section": "",
    "text": "Each of the headings below corresponds to a section in the table of contents on the left.\n\nAnatomy of a tree\n\nLearn how to read a tree, using data from the Titanic disaster.\n\nYour first tree\n\nThe basics of how to make a tree using Arbor.\n\n\n\n\nFbola example\n\nAnother example dataset, this time in a medical context. Use the tree to decide who probably has the virus.\n\nHow good is your tree?\n\nWhat makes a tree a good tree? Arbor can help you calculate tree quality.\n\nBreast cancer data\n\nWith a numerical predictor, we create our own measure of tree quality, and use it with graphs to make the best tree.\n\n\n\n\nThe configuration box\n\nWhen you “configure” a node in a tree, there is a dialog box. This explains all of its controls."
  },
  {
    "objectID": "10-arbor-anatomy.html#the-root-node",
    "href": "10-arbor-anatomy.html#the-root-node",
    "title": "2  Anatomy of a tree",
    "section": "2.1 The root node",
    "text": "2.1 The root node\nSo: way up at the top, the “root” block, the root node, looks like this:\n\n\n\n\n\nThe first horizontal “stripe” tells us what attribute (or variable) we are trying to predict. In this dataset, it’s called fate. (The value of fate is either survived or died.)\nThe second stripe tells us that we will consider survived to be the “positive” result. That seems obvious, but in many medical contexts, a “positive” test means that you have the disease."
  },
  {
    "objectID": "10-arbor-anatomy.html#the-trunk-node",
    "href": "10-arbor-anatomy.html#the-trunk-node",
    "title": "2  Anatomy of a tree",
    "section": "2.2 The trunk node",
    "text": "2.2 The trunk node\nJust below the root is the trunk. That block, that node, also has two stripes.\nThe top one tells you that 500 of the 1309 people—38.2% of them—survived. You know it’s survived because of what it says in the root. But if you forget, or get confused, you can always hover over that node to see details:\n\n\n\n\n\nThe bottom stripe sets up a branching. In this case, we ask about gender, and branch one way if the person is female and the other way if they are male."
  },
  {
    "objectID": "10-arbor-anatomy.html#other-nodes",
    "href": "10-arbor-anatomy.html#other-nodes",
    "title": "2  Anatomy of a tree",
    "section": "2.3 Other nodes",
    "text": "2.3 Other nodes\nEvery other node is structured more or less the same as the trunk node: the first stripe shows how many people are positive, that is, how many survived, and the second shows the branching if there is one.\nThe key thing about the number stripe is that it considers only the people who get to that point in the tree. So although 38% of all people survived, the tree shows us that there is a big gender difference: 73% of the females survived, but only 19% of the males.\nAnd then, among the males, the young males—under 15 years old—had a 50% survival rate compared to about 18% for males 15 or older."
  },
  {
    "objectID": "10-arbor-anatomy.html#terminal-nodes-the-leaves",
    "href": "10-arbor-anatomy.html#terminal-nodes-the-leaves",
    "title": "2  Anatomy of a tree",
    "section": "2.4 Terminal nodes: the leaves",
    "text": "2.4 Terminal nodes: the leaves\nIf a node has no branches coming out of it, it is a terminal node. Every terminal node has a “leaf” below it that is rounded instead of rectangular. The leaf shows a prediction for the fate of the people in that category.\nSo for the females, our best guess is that they will survive. For older males, our best guess is that they will die.\nFor the young males, under 15, with a 50% survival rate, we’re predicting survival, although you can make a case that at 50% we shouldn’t predict. The other side of that argument is that they have a better chance than people in general (38%)."
  },
  {
    "objectID": "10-arbor-anatomy.html#the-links",
    "href": "10-arbor-anatomy.html#the-links",
    "title": "2  Anatomy of a tree",
    "section": "2.5 The links",
    "text": "2.5 The links\nSlanted white stripes, called links, connect nodes to their “children.” Every link has a label such as female or >= 15 so you can tell who that link applies to.\nSo: that’s how you read a tree! To learn how to make a tree, see the page about learning to drive."
  },
  {
    "objectID": "20-arbor-begin.html#the-target-attribute",
    "href": "20-arbor-begin.html#the-target-attribute",
    "title": "3  Your first tree",
    "section": "3.1 The target attribute",
    "text": "3.1 The target attribute\nTo make a tree, you first have to identify your target attribute. The target attribute might also be called\n\nthe dependent variable\nthe outcome variable\nthe effect\n\nIt’s the thing you are trying to predict. In the case of the Titanic data, it’s fate: we want to know what makes it more likely to have survived the disaster.\nIn the live example below, start your tree by dragging fate from the table at the right and dropping it into the middle of the blank, gray tree panel. You should see that 500 of the 1309 people survived."
  },
  {
    "objectID": "20-arbor-begin.html#making-a-branch-drag-and-drop",
    "href": "20-arbor-begin.html#making-a-branch-drag-and-drop",
    "title": "3  Your first tree",
    "section": "3.2 Making a branch: drag and drop",
    "text": "3.2 Making a branch: drag and drop\nNext, drop gender onto the white box with “500 of 1309” in it.\nThe tree will branch. You can see how the survival rate was different for males and females.\nNotice that the tree is composed of boxes (called nodes) and lines (links).\n\n\n\n\n\n\nLost in the Node?\n\n\n\nIt’s easy to lose track of what’s going on in a node. When that happens, just point at the node. Don’t click, just hover for a moment, and text will pop up describing that node in more detail.\n\n\n\n\n\n\n\nThe pop-up information for the “male” node\n\n\n\n3.2.1 Adding a numeric attribute\nNow drop age onto the “male” node. It will split, but probably using age 30 as a cutpoint. Click the gear on the age stripe and change that 30 to 15.\n\n\n\n\n\nConfiguring age: setting the cutpoint to 15.\n\n\nWhen you make a tree with Arbor, every node has a maximum of two branches. An attribute like age has so many values, you generally have to tell Arbor where that cutpoint is using that configuration box.\nYou can configure any attribute, but it’s more common for numerical ones. Click to learn more about the configuration box.\n\n\n3.2.2 Assigning results to terminal nodes\n\n\n\n\n\nYour tree should look like this after you have assigned its leaf nodes to be survived or died.\n\n\nNow your tree should look like the one at right… except that you still have to assign the terminal nodes — the leaves of the tree — to an outcome. Do that by clicking on the leaf nodes repeatedly until you see what you want."
  },
  {
    "objectID": "30-arbor-with-graphs.html",
    "href": "30-arbor-with-graphs.html",
    "title": "4  Trees and graphs",
    "section": "",
    "text": "How should you decide how to build a tree? What attributes should you drag in? How do you pick a cut point?\nYou could just try things, but it often helps to make graphs: graphs that show the relationship between the attribute you’re wondering about and your target variable.\nFor example, here are two graphs of the same data showing the height of 28 professional athletes and what their sport is. In this case, all of these athletes play either rugby or basketball. That attribute, sport, is our target variable. That is, we are trying to use height to predict what sport they play.\n\n\n\n\n\n\nusing the vertical axis\n\n\n\n\n\n\n\nusing a legend\n\n\n\n\n\n\n\nTwo ways to show sport and height. As we might expect, basketball players are generally taller.\nWhich display is best? You will have to decide what works for you. Be sure you remember that you have a choice!\nWe could use either graph to decide on a cut point. It looks as if it’s something like 175 cm.\nIn the live example, experiment with different graphs. For example, do you think weight might be as good or better than height for separating basketball players from rugby players?\nBe sure to try making a graph with a legend. To make a legend (and color the points), plop an attribute in the middle of a graph.\nDon’t be afraid to make multiple graphs. You might want to minimize or shrink the tree to make room!\n\n\n\nTime for an exercise! In the live example make a tree…\n\n…that’s predicting sport\n…that branches on height\n…at a number near 175 cm\n…and that predicts that the taller athletes play basketball, and the rest rugby."
  },
  {
    "objectID": "40-arbor-fbola.html#the-same-tree-with-data",
    "href": "40-arbor-fbola.html#the-same-tree-with-data",
    "title": "5  Fbola example",
    "section": "5.1 The same tree, with data",
    "text": "5.1 The same tree, with data\nThat tree was made from intuition. We can also make it with data, using Arbor. Suppose that we take 100 students and give them an expensive, time-consuming test for Fbola; those test results appear in the Fbola column. We also record whether they have a rash or a fever.\nUse the live example below to make the tree. Don’t hesitate to make graphs to see relationships between the attributes.\nYou can even consider making a different tree to accomplish the same task—by asking about the fever first.\nDon’t forget to include diagnoses at the ends of all your branches!"
  },
  {
    "objectID": "40-arbor-fbola.html#trees-as-models",
    "href": "40-arbor-fbola.html#trees-as-models",
    "title": "5  Fbola example",
    "section": "5.2 Trees as models",
    "text": "5.2 Trees as models\nNotice this very very important fact: our diagnosis might be wrong. A positive diagnosis using the tree might be a true positive (TP), that is, it’s correct and you have the disease; or it might be a false positive (FP), that is, we send you home even though you are well.\nSimilarly, you can get false negatives (FN) and true negatives (TN).\nYou can see all four possibilities by looking at the table tab in Arbor. (Do that!) A typical tree, with its corresponding table, looks like this:\n\n\n\n\n\n\n\n\n\nThe 14 false positives combine seven from the leftmost leaf and seven from the middle one.\n\n\n\n\n\nThis leads us to an issue we have to emphasize: how do we know whether our diagnosis is right or wrong? The answer to that is, sometimes we don’t. In this case, though, we assume that the time-consuming and expensive test is perfectly accurate. In our practical situation at school, however, we hope our rash-and-fever tree does a good enough job.\nIn general, when we have a classification problem like this, there is some underlying Truth that we cannot see. We can only see the shadow3 of this Truth, in the form of data. We see the symptoms, not the actual disease. We use the data to make our best guess about the Truth.\nIn our case, we are trying to predict Fbola—the results of the expensive test—using symptoms: the data about rash and fever.\nTaken together, this all means that a tree is a model. It’s an approximation of the truth that we will make as useful as possible. But it’s not the Truth; it’s a human construct.\nAlso, math nerds, notice that in this model, the tree’s procedure is a function. Its inputs are the data (fever and rash) and the inevitable output is either positive or negative. Notice how this is parallel to the situation when you use a line as an approximation to data in a scatter plot. The line is a function, and it’s not completely correct even though it can be useful.\nIn that situation, we can even try to find the best line using a criterion such as least squares. And that’s whats coming next with our lessons on trees."
  },
  {
    "objectID": "50-arbor-measures.html#constructing-a-measure",
    "href": "50-arbor-measures.html#constructing-a-measure",
    "title": "6  How good is your tree?",
    "section": "6.1 Constructing a measure",
    "text": "6.1 Constructing a measure\nLet’s look at one common measure, called the misclassification rate, which I will abbreviate MCR. That’s equal to\n\\[\\rm{MCR} = \\frac{\\textrm{number of wrong diagnoses}}{\\textrm{number of diagnoses}}\\]\nArbor can supply you with various numbers you can combine to make your measure. These include the number of true positives (\\(\\rm{TP}\\)), false negatives (\\(\\rm{FN}\\)), and so forth. With those numbers, you could calculate the MCR like this:\n\\[\\rm{MCR} = \\frac{FP+FN}{TP+TN+FP+FN}\\]\nThe plan, then, would be to make a formula for MCR—or whatever measure you want—in CODAP. But where would you make that formula? And how would you get the attributes like TP to put in it?"
  },
  {
    "objectID": "50-arbor-measures.html#getting-fp-and-tn-and-all-that",
    "href": "50-arbor-measures.html#getting-fp-and-tn-and-all-that",
    "title": "6  How good is your tree?",
    "section": "6.2 Getting FP and TN and all that",
    "text": "6.2 Getting FP and TN and all that\nAs you have seen, these numbers appear at the bottom of your tree. You could copy them and type them in, but you don’t have to.\nLet’s revisit the Fbola example. The live illustration below shows a tree that uses only the rash attribute as a predictor. In the context of the example, that means we send everybody home who has a rash, and do not take their temperature.\nAs you can see from the values below the tree, among the 100 people, we have 7 false positives (FP=7) and 12 false negatives. That means that the misclassification rate, MCR, is (7 + 12)/100, or 0.19.\n\n\n\nWe don’t want to do that calculation by hand every time, so do this:\n\nClick the disclosure triangle just left of in order to export. Some controls appear. Ignore most of them!\nPress the emit data button.\n\nAha! A new table appears called Classification Tree Records. You can see that it has already calculated MCR and (scroll right…) reports values for TP, FN, and the rest as well as N (the total number) and potentially useful quantities such as the number of nodes altogether and the depth of the tree.\nLet’s change the tree so we can compare!\n\nDrag fever in and drop it on the left-hand node.\nGive the two “vacant” leaves appropriate values.\nClick emit data again.\n\nYour new table should now look like this:\n\nAccording to the MCRs—where we want a small value—the new tree is better. The sensitivity (sens) in the table is also better (we want it to be large).\nNow. Arbor comes with MCR and sens pre-defined. But they are only CODAP columns with formulas. This means that you can make new columns inth is table and define any possible measure of quality for your trees."
  },
  {
    "objectID": "50-arbor-measures.html#which-tree-made-this",
    "href": "50-arbor-measures.html#which-tree-made-this",
    "title": "6  How good is your tree?",
    "section": "6.3 Which tree made this?",
    "text": "6.3 Which tree made this?\nThe best for last: As you might imagine, after you’ve made a few trees and emitted the data, you might not reemmber exactly what tree created which line in the table.\nDon’t worry: Arbor has your back.\nSimply click on the row you want to know about in the Records table, and Arbor will restore that tree.\nWe will continue with this topic in a bit more depth in the next example, using breast-cancer data."
  },
  {
    "objectID": "60-arbor-breast-cancer.html#looking-at-the-data",
    "href": "60-arbor-breast-cancer.html#looking-at-the-data",
    "title": "7  More about tree quality: an example with breast cancer data",
    "section": "7.1 Looking at the data",
    "text": "7.1 Looking at the data\nIn the live illustration below, you already have a tree set to predict biopsy.\n\nLook at the tree: among these 569 tumors, 37.3% were malignant.\nLook at the graph: apparently (and unsurprisingly) benign tumors are generally smaller.\nAlso in the graph: we have a “movable value” currently set to 16.6 millimeters. Almost all of the tumors bigger than 16.5 mm are malignant.\n\n\n\n\n\n\n\nCan’t see the table?\n\n\n\nThe table—collapsed into “case card view”—may be hiding behind the tree. Make the tree narrower.\n\n\nDo the following:\n\nDrop radius into the trunk of the tree to make a branch.\n\nArbor will pick a cut point that determines which values of radius go to the larger branch and which go to the smaller. When I did this, that cut point was 26 mm. You need to change that to be 16.5:\n\nHover over the radius node and click the gear that appears. The configuration box appears. (Learn more about the configuration box.)\n\n\n\n\n\nChange the value in the lower box from 26 to 16.5 and press Done.\nSpecify diagnoses for your leaves: make the “large” tumors Malignant.\n\n\n\n\nIf your setup is like mine, that will give you 125 malignant diagnoses, of which 122 were correct. So that leaves three false positives. On the other hand, there are 90 false negatives. That’s a lot!\nLet’s alter the graph to see this more clearly.\n\nDrop biopsy onto the vertical axis of the graph.\n\nThe graph splits to make two parallel dot plots, each with its own movable value. There are now four counts (and percentages); one for TP, one for FP, etc. See if you can clearly identify which is which, and why. Your graph should look like this:\n\n\n\nIf everything is working correctly, your graph should look like this.\n\n\nWe want to figure out what a good cut point would be. You can explore changing the movable values from 16.5 (you have to change both of them) and see how that affects the numbers of FP (upper right) and FN (lower left). And you should notice two sad truths:\n\nBecause the two distributions overlap, there is no way to get both FP = 0 and FN = 0.\nFalse negatives are the worst. We want to lower that number. But if you lower the cutpoint value to decrease FN, FP has to increase.\n\nSuppose we wanted to eliminate FN’s altogether. We would set the cutpoint to some small number (like zero)…but then all tumors would be diagnosed as malignant. No false negatives, because there would be no negatives at all!\nThis means that every cutpoint is a balancing act. We can use a measure of goodness of a tree to help us achieve that balance.\nWe could make a measure of our tree’s effectiveness just like we did in the previous section. Suppose we decide that a false negative is five times as bad as a false positive. Then our formula might be\nFN * 5 + FP\nand we would want to minimize that value.\nRemember that to get Arbor to calculate the values,\n\nOpen up the in order to export section below the tree.\nPress emit data.\n\nThat will give you a new table with values for FP etc. Then,\n\nMake a new attribute (perhaps rating) and give it that formula, FN * 5 + FP. The value you’ve calculated appears in the table.\nMake another new attribute (perhaps cutpoint) and enter the value for the cut point (otherwise you might forget).\n\nYour summary table might look like this:\n\n\n\nOur “emitted data” table, with two new columns at the far right."
  },
  {
    "objectID": "60-arbor-breast-cancer.html#some-tasks",
    "href": "60-arbor-breast-cancer.html#some-tasks",
    "title": "7  More about tree quality: an example with breast cancer data",
    "section": "7.2 Some tasks",
    "text": "7.2 Some tasks\nHere are some tasks you might do to extend your understanding.\n\nFind the minimum\nTry various cutpoints and see which one gives the lowest value for the measure.\n\nChange the value of the cutpoint using the configuration box\nRepeat the instructions above: emit data and enter the value of the cutpoint. The FN * 5 + FP will be calculated automatically.\nMake a graph of that value as a function of the cutpoint. Is there a minimum? (Yes!)\n\nNote: the movable values in the graph are not connected to the cut point!\n\n\nTry other attributes\nSee how you can do putting other attributes besides radius in the tree.\nDon’t forget to help yourself out by making graphs!"
  },
  {
    "objectID": "90-arbor-configuration.html#left-and-right",
    "href": "90-arbor-configuration.html#left-and-right",
    "title": "8  Configuring a node: the details",
    "section": "8.1 Left and right",
    "text": "8.1 Left and right\nThe double-headed arrow diamond near the lower right is a button that reverses the node. With that control, you can (for example) set up every branching so that the more “positive” result flows to the left. A tree is less confusing if the results are less mixed up."
  },
  {
    "objectID": "90-arbor-configuration.html#cutpoints",
    "href": "90-arbor-configuration.html#cutpoints",
    "title": "8  Configuring a node: the details",
    "section": "8.2 Cutpoints",
    "text": "8.2 Cutpoints\n\n\n\nSample configuration box for age\n\n\nIf the attribute is numeric, you have to decide what number separates the positive from the negative values.\nEnter the number you want for the cutpoint and use the menu to choose the operator that governs which value(s) go on the left.\nNotice that you can use equality. This is useful when you want to isolate a single value for some reason."
  },
  {
    "objectID": "90-arbor-configuration.html#more-than-two-categorical-values",
    "href": "90-arbor-configuration.html#more-than-two-categorical-values",
    "title": "8  Configuring a node: the details",
    "section": "8.3 More than two categorical values",
    "text": "8.3 More than two categorical values\nSuppose you have four values in the columns, such as Freshman, Sophomore, Junior, and Senior. By default, Arbor picks one value for the left side, and puts the rest on the right:\n\n\n\n“Before.” Frosh are alone on the left.\n\n\nNow suppose you want to split a node by whether the cases are upper or lower class.\nJust click on a value to move it to the other side. For example, clicking on the Soph button will move it to the left side. It will look like this, although you have to edit the labels as we did:\n\n\n\n“After.” Frosh and sophomores are together on the left."
  },
  {
    "objectID": "90-arbor-configuration.html#labels",
    "href": "90-arbor-configuration.html#labels",
    "title": "8  Configuring a node: the details",
    "section": "8.4 Labels",
    "text": "8.4 Labels\nThe text in the Labels boxes appears on the tree, attached to the links to the “child” nodes.\nEnter whatever you want, in order to make the tree clear and correct.\nIf you have a numerical attribute and a cutpoint, Awash automatically fills in the label with an operator and a value."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Before the CART",
    "section": "",
    "text": "These pages are an online supplement for a paper in Teaching Statistics called What goes before the CART? Introducing classification trees with Arbor and CODAP by Joachim Engel and Tim Erickson.\nThese pages give you, the reader, a chance to build trees using the Arbor tool and get a feel for using CODAP. We have also incorporated this material into a much larger online book, Awash in Data.\nCODAP is a free, web-based data analysis platform. Arbor is a CODAP plugin that helps you make classification trees.\nTo learn CODAP basics, here are two strategies:\n\nWork through the getting started lesson in the “Awash” book.\nOpen CODAP, choose to get a sample document, and choose Getting started with CODAP."
  },
  {
    "objectID": "50-arbor-measures.html#other-famous-measures",
    "href": "50-arbor-measures.html#other-famous-measures",
    "title": "6  How good is your tree?",
    "section": "6.4 Other famous measures",
    "text": "6.4 Other famous measures\nWe’ve defined MCR, the misclassification rate, and alluded to sens, the sensitivity. Here are a few of the most “famous” measures and what they mean:\n::: {.column-page-right} {#sensitivity-definition}\n\n\n\n\nname\n\n\ndescription\n\n\nformula\n\n\n\n\nmcr\n\n\nmisclassification rate: the fraction of all diagnoses that are incorrect\n\n\n\\[\\frac{FP+FN}{TP+TN+FP+FN}\\]\n\n\n\n\nsens\n\n\nsensitivity: The proportion of all actually positive cases that test positive, i.e., if you’re actually sick, what’s the chance that the test says so?\n\n\n\\[\\frac{TP}{TP+FN}\\]\n\n\n\n\nspec\n\n\nspecificity: The proportion of all actually negative cases that test negative.\n\n\n\\[\\frac{TN}{TN+FP}\\]\n\n\n\n\nppv\n\n\npositive predictive value or precision: The proportion of positive diagnoses that are correct, i.e., if you get a positive test, what’s the chance that you’re actually sick?\n\n\n\\[\\frac{TP}{FP+TP}\\]\n\n\n\n\n:::"
  },
  {
    "objectID": "50-arbor-measures.html#sensitivity-definition",
    "href": "50-arbor-measures.html#sensitivity-definition",
    "title": "6  How good is your tree?",
    "section": "6.4 Other famous measures",
    "text": "6.4 Other famous measures\nWe’ve defined MCR, the misclassification rate, and alluded to sens, the sensitivity. Here are a few of the most “famous” measures and what they mean; you can of course make new columns in CODAP to compute any of these, using TP, FN, and the rest:\n\n\n\n\n\nname\n\n\ndescription\n\n\nformula\n\n\n\n\nmcr\n\n\nmisclassification rate: the fraction of all diagnoses that are incorrect\n\n\n\\[\\frac{FP+FN}{TP+TN+FP+FN}\\]\n\n\n\n\nsens\n\n\nsensitivity: The proportion of all actually positive cases that test positive, i.e., if you’re actually sick, what’s the chance that the test says so?\n\n\n\\[\\frac{TP}{TP+FN}\\]\n\n\n\n\nspec\n\n\nspecificity: The proportion of all actually negative cases that test negative.\n\n\n\\[\\frac{TN}{TN+FP}\\]\n\n\n\n\nppv\n\n\npositive predictive value or precision: The proportion of positive diagnoses that are correct, i.e., if you get a positive test, what’s the chance that you’re actually sick?\n\n\n\\[\\frac{TP}{FP+TP}\\]\n\n\n\n\n\nTo find even more measures—it will make you dizzy—check out a suitable Wikipedia page.\nHowever: who needs to learn these famous (and often confusing) measures? Drug manufacturers. Epidemiologists. Pharmacists. People who read papers by all of the above. But when you’re just starting to think about classification and data, we think it’s much more powerful to design your own measure for your own purpose: one like our FN * 5 + FP, that expresses our particular concern. In this case, that was that false negatives are worse than false positives—but it will be different in different contexts."
  }
]