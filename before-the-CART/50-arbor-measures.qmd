{{< include _awash-setup.qmd  >}}

# How good is your tree?

Now we come to the issue of the *quality* of a tree. 
If you've made a tree, how good is it? 
If you have two versions of a tree, which is better?

Of course, if a tree is perfect---it gives a correct diagnosis every time---there
is no problem deciding if it's good enough. 
It is.
But if you've stayed with us this far, 
you have seen that the prediction a tree gives is not always correct.

So: to compare two trees, we need a *measure* of tree quality.
You will pick the tree with the best value for that measure.

The problem is, what measure should you pick?
With *Arbor*, you can construct various measures and then ask yourself 
whether the measure you choose (or design) 
faithfully represents what you think is important in your context.

## Constructing a measure

Let's look at one common measure, called the *misclassification rate*, which I will abbreviate MCR.
That's equal to

$$\rm{MCR} = \frac{\textrm{number of wrong diagnoses}}{\textrm{numberÂ of diagnoses}}$$


*Arbor* can supply you with various numbers you can combine to make your measure. 
These include the number of true positives ($\rm{TP}$), false negatives ($\rm{FN}$), and so forth.
With those numbers, you could calculate the MCR like this:

$$\rm{MCR} = \frac{FP+FN}{TP+TN+FP+FN}$$

The plan, then, would be to make a formula for `MCR`---or whatever measure you want---in CODAP.
But where would you make that formula?
And how would you get the attributes like `TP` to put in it?

## Getting FP and TN and all that

As you have seen, these numbers appear at the bottom of your tree. 
You *could* copy them and type them in, but you don't have to.

Let's revisit the Fbola example. 
The live illustration below shows a tree that uses only the `rash` attribute as a predictor.
In the context of the example, that means we send everybody home who has a rash,
and do not take their temperature.

As you can see from the values below the tree, among the 100 people, 
we have 7 false positives (`FP=7`) and 12 false negatives.
That means that the misclassification rate, MCR, is (7 + 12)/100, or 0.19. 

::: {.column-page-right}
<script>
    const theURL = "https://codap.concord.org/releases/latest/static/dg/en/cert/index.html#shared=https%3A%2F%2Fcfm-shared.concord.org%2FYJqHZ3h9qtWyUFB0GyWS%2Ffile.json";
    awash.liveDemoAt(theURL);
</script>
:::

We don't want to do that calculation by hand every time, so do this:

(@)  Click the disclosure triangle just left of **in order to export**. 
Some controls appear. Ignore most of them!

(@) Press the **emit data** button. 

Aha! A new table appears called **Classification Tree Records**. 
You can see that it has already calculated `MCR` and (scroll right...) 
reports values for `TP`, `FN`, and the rest as well as `N` (the total number)
and potentially useful quantities such as the number of nodes altogether and the depth of the tree. 

Let's change the tree so we can compare!

(@) Drag `fever` in and drop it on the left-hand node.

(@) Give the two "vacant" leaves appropriate values.

(@) Click **emit data** again. 

Your new table should now look like this:

![](art/measures-two-results.png)

According to the `MCR`s---where we want a *small* value---the new tree is better.
The [sensitivity](#sensitivity-definition)
(`sens`) in the table is also better (we want it to be large). 

Now. 
Arbor comes with `MCR` and `sens` pre-defined. 
But they are only CODAP columns with formulas.
This means that you can make new columns inth is table and define *any possible* 
measure of quality for your trees. 

## Which tree made this?

The best for last: As you might imagine, after you've made a few trees and emitted the data,
you might not reemmber exactly what tree created which line in the table.

Don't worry: *Arbor* has your back. 

Simply click on the row you want to know about in the **Records** table,
and *Arbor* will restore that tree.

We will continue with this topic in a bit more depth 
in [the next example, using breast-cancer data](#breast-cancer-chapter).

## Other famous measures {#sensitivity-definition}

We've defined `MCR`, the misclassification rate,
and alluded to `sens`, the sensitivity.
Here are a few of the most "famous" measures and what they mean;
you can of course make new columns
in CODAP to compute any of these,
using `TP`, `FN`, and the rest:

::: {.column-page-right} 

<table>
<tr>
<th>name</th><th>description</th><th>formula</th>
</tr>

<tr>
<td>`mcr`</td>
<td>**misclassification rate**: the fraction of all diagnoses 
that are incorrect</td>
<td>$$\frac{FP+FN}{TP+TN+FP+FN}$$</td>
</tr>

<tr>
<td>`sens`</td>
<td>**sensitivity**: 
The proportion of all actually positive cases that test positive, i.e., if you're actually sick, what's the chance that the test says so?
</td>
<td>$$\frac{TP}{TP+FN}$$</td>
</tr>

<tr>
<td>`spec`</td>
<td>**specificity**: 
The proportion of all actually negative cases that test negative.
</td>
<td>$$\frac{TN}{TN+FP}$$</td>
</tr>

<tr>
<td>`ppv`</td>
<td>**positive predictive value** or **precision**: 
The proportion of positive diagnoses that are correct, i.e., if you get a positive test, what's the chance that you're actually sick?
</td>
<td>$$\frac{TP}{FP+TP}$$</td>
</tr>

</table>
:::

To find even more measures---it will make you dizzy---check
out a [suitable Wikipedia page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).

However: who needs to learn these famous (and often confusing) measures?
Drug manufacturers. Epidemiologists.
Pharmacists. People who read papers by all of the above.
But when you're just *starting* to think about classification
and data, we think it's much more powerful
to design your own measure for your own purpose:
one like our `FN * 5 + FP`,
that expresses our particular concern. 
In this case, that was that false negatives are worse
than false positives---but it will be different 
in different contexts.