{{< include _awash-setup.qmd  >}}

# Trees and machine learning

One reason to learn about trees is that they are a topic in Machine Learning (ML),
and that topic is a hot one right now. 
So thinking deeply about trees helps you understand what ML is all about.

This is not the place for a treatise on machine learning. 
But I will natter on for a bit describing some of the connections I see. 
If ML is new to you, it should give you an idea of at least one part of ML and where this fits. 

So: what's the connection between classification trees and machine learning? 

If you have worked through all of the pages before this one,
you've seen that, although you can make pretty good trees just using your intuition,
there are ways to assign numbers to the quality of the trees you make,
and informally optimize a tree. 
That is, you can make a plausible tree, and then alter it---add
another branch, change a break point---and gradually find the best one.

If you do that enough, it probably will occur to you that this is tedious,
and wonder if we could have a computer do it.

This business of looking at a set of trees,
then finding the best one, 
and maybe then using that to make another, generally better set of trees,
and repeating that process,
well, that's an example of machine learning. 
It's creating an *algorithm* to find the best---or at least a pretty good---version of something. 

## Imagining automating tree-building

It's one thing to say, we'll have the computer do the boring part. 
It's another to specify exactly what to have the computer do. 
We will not do that here completely, but we will talk about what's required for that task.

### The branch-*everything* strategy

First, let's imagine a brute-force procedure: 
make the biggest tree possible.
To do that, take the first "predictor" attribute that you see,
and branch the tree using that attribute.
Now you have two terminal nodes.
Then take the next attribute, and branch both terminal nodes;
now you have four.
Continue until you're out of attributes.
(If you start with $n$ attributes, 
you will wind up with $2^n$ terminal nodes...which can be a lot!)

To do that, you still need three things:

* At the beginning, you need a target attribute 
and you need to know what value(s) of that attribute are "positive."

* You need to know, for each attribute, if it has more than two values, how to split it: You need a splitting rule---perhaps a cut point.

* At the end, you need to know, for each terminal node,
what diagnosis you will assign. 

Your algorithm will have to address those issues.
Then, if you're about to make the *last* branch, 
you will notice situations where it makes no sense 
to branch the tree, for example:

* when the split makes almost no difference 
in the percentage of cases that are positive. 

* when the number of cases in a branch is small 

So to make a "trimmer" tree, your algorithm will need
to specify what "almost no difference" or "small" mean.

You might also look at your completed tree and decide
to "prune" it---to eliminate additional branches
that yield little difference or have too few cases. 

### A more prudent and frugal strategy

An alternative to branching everything is to be more
careful about what branches you make. 
Imagine these steps:

1. Take all the attributes and make a tree for each one,
with just that first split.

2. For each tree, calculate how good the tree is
using some measure such as the MCR (misclassification rate).

3. Take the best tree as measured by that MCR,
returning to step 1 using only the remaining 
attributes. 

4. If you ever get a split that's too little difference or 
too small, stop branching that part of the tree.

You still need to define many things, 
but this will generally yield smaller, 
more wieldy trees. 


### Linear regression metaphor

Let's assume we all understand about least-squares linear regression,
a process by which, given a set of data points, we find a line that minimizes 
the sum of the squares of the residuals.
We compute the parameters of that line---the slope and intercept, $m$ and $b$---using some formulas.
And those formulas get derived using calculus.^[This is not surprising,
because this is an optimization problem with quadratic functions (sum of *squares*) so it
involves setting a derivative equal to zero...and so forth.]

Now let's imagine that we don't have calculus. 

We can still solve the problem using an iterative process. 
Maybe we begin using the function $m=0$ and $b=0$, that is, $y=0x+0$.
We can compute the sum of squares of the residuals. 
Then, at every step, we look at new values of $m$ and $b$, offset a little from their current values,
and calculate the sum of squares from new lines defined by the new values. 
We pick the line with the lowest sum of squares and then do it all over again.
Gradually, when we get close, we can reduce the offsets, and thereby find
an optimum line to as great a precision as you like. 

You can even imagine this as walking on a *surface*.
The $m$ and $b$ axes define a horizontal plane,
and the height ($z$) of the surface is the sum of squares for each location $(m, b)$.
Our task is to find the lowest point on the surface.

Calculus does it immediately,
but we have the computing power to do the "walk" very efficiently and quickly.
And of course, this procedure---which is called *gradient descent* in ML-speak---works even when calculus does not. 



cost: entropy, gini instead of SSR

training and test sets

overfitting
